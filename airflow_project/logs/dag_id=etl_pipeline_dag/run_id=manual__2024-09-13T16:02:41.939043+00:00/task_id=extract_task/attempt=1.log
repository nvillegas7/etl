[2024-09-13T16:02:42.812+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: etl_pipeline_dag.extract_task manual__2024-09-13T16:02:41.939043+00:00 [queued]>
[2024-09-13T16:02:42.819+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: etl_pipeline_dag.extract_task manual__2024-09-13T16:02:41.939043+00:00 [queued]>
[2024-09-13T16:02:42.819+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2024-09-13T16:02:42.819+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 1
[2024-09-13T16:02:42.819+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2024-09-13T16:02:42.836+0000] {taskinstance.py:1304} INFO - Executing <Task(PythonOperator): extract_task> on 2024-09-13 16:02:41.939043+00:00
[2024-09-13T16:02:42.839+0000] {standard_task_runner.py:55} INFO - Started process 232 to run task
[2024-09-13T16:02:42.841+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'etl_pipeline_dag', 'extract_task', 'manual__2024-09-13T16:02:41.939043+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/tmp/tmp5x_lf0es']
[2024-09-13T16:02:42.951+0000] {standard_task_runner.py:83} INFO - Job 40: Subtask extract_task
[2024-09-13T16:02:42.960+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-09-13T16:02:42.991+0000] {task_command.py:389} INFO - Running <TaskInstance: etl_pipeline_dag.extract_task manual__2024-09-13T16:02:41.939043+00:00 [running]> on host 2656b950921d
[2024-09-13T16:02:43.038+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_pipeline_dag
AIRFLOW_CTX_TASK_ID=extract_task
AIRFLOW_CTX_EXECUTION_DATE=2024-09-13T16:02:41.939043+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2024-09-13T16:02:41.939043+00:00
[2024-09-13T16:02:43.075+0000] {logging_mixin.py:137} WARNING - /home/***/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:1052 InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
[2024-09-13T16:02:43.153+0000] {logging_mixin.py:137} INFO - Data extracted successfully
[2024-09-13T16:02:43.154+0000] {python.py:177} INFO - Done. Returned value was:       FIPS Admin2  ... Incident_Rate Case_Fatality_Ratio
0      NaN    NaN  ...    134.896578            4.191343
1      NaN    NaN  ...   2026.409062            2.025173
2      NaN    NaN  ...    227.809861            2.764848
3      NaN    NaN  ...  10505.403482            1.034865
4      NaN    NaN  ...     53.452981            2.305328
...    ...    ...  ...           ...                 ...
4006   NaN    NaN  ...      0.000000            0.000000
4007   NaN    NaN  ...      0.000000            0.000000
4008   NaN    NaN  ...      0.000000            0.000000
4009   NaN    NaN  ...      0.000000            0.000000
4010   NaN    NaN  ...      0.000000            0.000000

[4011 rows x 14 columns]
[2024-09-13T16:02:43.175+0000] {xcom.py:635} ERROR - Object of type DataFrame is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config or make sure to decorate your object with attr.
[2024-09-13T16:02:43.177+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2301, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 240, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 627, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/local/lib/python3.7/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 176, in encode
    return super().encode(o)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 170, in default
    return super().default(o)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2024-09-13T16:02:43.185+0000] {taskinstance.py:1327} INFO - Marking task as FAILED. dag_id=etl_pipeline_dag, task_id=extract_task, execution_date=20240913T160241, start_date=20240913T160242, end_date=20240913T160243
[2024-09-13T16:02:43.198+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 40 for task extract_task (Object of type DataFrame is not JSON serializable; 232)
[2024-09-13T16:02:43.215+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2024-09-13T16:02:43.329+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
